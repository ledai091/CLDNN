{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional\n",
    "from torch import Tensor\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import torchvision\n",
    "from glob import glob\n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "import re\n",
    "from torch.optim import AdamW \n",
    "from torch.nn import CrossEntropyLoss\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import v2\n",
    "import pandas as pd\n",
    "from torchvision.transforms import CenterCrop\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from timm import create_model\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, device: torch.device) -> None:\n",
    "        super(DNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc1 = nn.Linear(input_size, 200).to(device)\n",
    "        self.bn1 = nn.BatchNorm1d(200).to(device)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(200, 100).to(device)\n",
    "        self.bn2 = nn.BatchNorm1d(100).to(device)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(100, output_size).to(device)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, bias: bool = True, device: torch.device = None) -> None:\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        self.weight_ih = nn.Parameter(torch.empty(4*hidden_size, input_size, device=device))\n",
    "        self.weight_hh = nn.Parameter(torch.empty(4*hidden_size, hidden_size, device=device))\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "        if self.bias:\n",
    "            self.bias_ih = nn.Parameter(torch.empty(4*hidden_size, device=device))\n",
    "            self.bias_hh = nn.Parameter(torch.empty(4*hidden_size, device=device))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "\n",
    "        self.peephole_i = nn.Parameter(torch.empty(hidden_size, device=device))\n",
    "        self.peephole_f = nn.Parameter(torch.empty(hidden_size, device=device))\n",
    "        self.peephole_o = nn.Parameter(torch.empty(hidden_size, device=device))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.orthogonal_(self.weight_hh)\n",
    "        nn.init.xavier_normal_(self.weight_ih)\n",
    "        nn.init.constant_(self.bias_hh, 0)\n",
    "        nn.init.constant_(self.bias_ih, 0)\n",
    "        nn.init.normal_(self.peephole_i, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.peephole_f, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.peephole_o, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, input: Tensor, state: Tuple[Tensor, Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        hx, cx = state\n",
    "        gates = torch.mm(input, self.weight_ih.t()) + torch.mm(hx, self.weight_hh.t())\n",
    "        if self.bias:\n",
    "            gates += self.bias_ih + self.bias_hh\n",
    "\n",
    "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
    "\n",
    "        input_gate = torch.sigmoid(input_gate + self.peephole_i * cx)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self.peephole_f * cx)\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "\n",
    "        cy = (forget_gate * cx) + (input_gate * cell_gate)\n",
    "\n",
    "        output_gate = torch.sigmoid(output_gate + self.peephole_o * cy)\n",
    "\n",
    "        hy = output_gate * torch.tanh(cy)\n",
    "        return hy, cy\n",
    "\n",
    "class LSTMBlock(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_cells: int, device: torch.device) -> None:\n",
    "        super(LSTMBlock, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_cells = num_cells\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([LSTMCell(self.input_size, hidden_size, device=device)\n",
    "                                         if i == 0\n",
    "                                         else LSTMCell(self.hidden_size, self.hidden_size, device=device)\n",
    "                                         for i in range(self.num_cells)])\n",
    "\n",
    "    def forward(self, input: Tensor, state: Tuple[Tensor, Tensor]=None)->Tuple[Tensor, Tensor]:\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        if state is None:\n",
    "            zeros = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
    "            state = (zeros, zeros)\n",
    "\n",
    "        hx, cx = state\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        seq_len = input.size(1)\n",
    "        for t in range(seq_len):\n",
    "            x = input[:, t, :]\n",
    "            for i, lstm_cell in enumerate(self.lstm_cells):\n",
    "                hx, cx = lstm_cell(x, (hx, cx))\n",
    "                x = hx\n",
    "            outputs.append(hx)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, (hx, cx)\n",
    "\n",
    "class BiLSTMBlock(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_cells: int, device: torch.device) -> None:\n",
    "        super(BiLSTMBlock, self).__init__()\n",
    "        self.forward_lstm = LSTMBlock(input_size, hidden_size // 2, num_cells, device)\n",
    "        self.backward_lstm = LSTMBlock(input_size, hidden_size // 2, num_cells, device)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        forward_output, _ = self.forward_lstm(input)\n",
    "        backward_output, _ = self.backward_lstm(torch.flip(input, [1]))\n",
    "        backward_output = torch.flip(backward_output, [1])\n",
    "        return torch.cat((forward_output, backward_output), dim=2)\n",
    "\n",
    "class MultiBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_blocks: int, num_cells_per_block: int, device: torch.device) -> None:\n",
    "        super(MultiBiLSTM, self).__init__()\n",
    "        self.device = device\n",
    "        self.blocks = nn.ModuleList([\n",
    "            BiLSTMBlock(input_size, hidden_size, num_cells_per_block, device)\n",
    "            if i == 0\n",
    "            else BiLSTMBlock(hidden_size, hidden_size, num_cells_per_block, device)\n",
    "            for i in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, output_size: int, device: torch.device) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1).to(device)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1).to(device)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1).to(device)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1).to(device)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, output_size).to(device)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 cnn_out_channels: int,\n",
    "                 lstm_input_size: int,\n",
    "                 lstm_hidden_size: int,\n",
    "                 lstm_num_blocks: int,\n",
    "                 lstm_num_cells_per_block: int,\n",
    "                 dnn_output_size: int,\n",
    "                 device: torch.device) -> None:\n",
    "        super(CLDNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.cnn = CNN(in_channels, cnn_out_channels, device)\n",
    "        self.lstm = MultiBiLSTM(lstm_input_size, lstm_hidden_size, lstm_num_blocks, lstm_num_cells_per_block, device)\n",
    "        self.dnn = DNN(lstm_hidden_size, dnn_output_size, device)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "\n",
    "        cnn_out = []\n",
    "        for t in range(seq_len):\n",
    "            img = x[:, t, :, :, :]\n",
    "            cnn_out.append(self.cnn(img))\n",
    "            \n",
    "        cnn_out = torch.stack(cnn_out, dim=1)\n",
    "        \n",
    "        lstm_out = self.lstm(cnn_out)\n",
    "        \n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        out = self.dnn(last_output)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "in_channels = 3\n",
    "cnn_out_channels = 256\n",
    "lstm_input_size = cnn_out_channels\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_blocks = 5\n",
    "lstm_num_cells_per_block = 5\n",
    "dnn_output_size = 2\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLDNN(\n",
    "    in_channels=in_channels,\n",
    "    cnn_out_channels=cnn_out_channels,\n",
    "    lstm_input_size=lstm_input_size, \n",
    "    lstm_hidden_size=lstm_hidden_size, \n",
    "    lstm_num_blocks=lstm_num_blocks,\n",
    "    lstm_num_cells_per_block=lstm_num_cells_per_block,\n",
    "    dnn_output_size=dnn_output_size,\n",
    "    device=device)\n",
    "# optimizer = AdamW(model.parameters(), lr = 0.001)\n",
    "# loss_fn = CrossEntropyLoss()\n",
    "# epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort_key(filename):\n",
    "    match = re.search(r'frame_(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return filename  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('weight/train2/last.pt'))\n",
    "path = 'img_resize/Thang3/2_220474_well02_zid99_30'\n",
    "list_path_img = sorted(glob(os.path.join(path, '*.jpg')), key=custom_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_0.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_1.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_2.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_3.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_4.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_5.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_6.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_7.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_8.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_9.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_10.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_11.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_12.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_13.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_14.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_15.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_16.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_17.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_18.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_19.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_20.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_21.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_22.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_23.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_25.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_26.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_27.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_28.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_29.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_30.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_31.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_32.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_33.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_34.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_35.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_36.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_37.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_38.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_39.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_40.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_41.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_42.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_43.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_44.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_45.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_46.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_47.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_48.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_49.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_50.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_51.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_52.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_53.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_54.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_55.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_56.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_57.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_58.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_59.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_60.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_61.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_62.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_63.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_64.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_65.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_66.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_67.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_68.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_69.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_70.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_71.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_72.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_73.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_74.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_75.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_76.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_77.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_78.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_79.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_80.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_81.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_82.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_83.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_84.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_85.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_86.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_87.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_88.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_89.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_90.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_91.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_92.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_93.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_94.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_95.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_96.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_97.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_98.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_99.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_100.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_101.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_102.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_103.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_104.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_105.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_106.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_107.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_108.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_109.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_110.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_111.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_112.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_113.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_114.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_115.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_116.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_117.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_118.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_119.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_120.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_121.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_122.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_123.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_124.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_125.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_126.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_127.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_128.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_129.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_130.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_131.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_132.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_133.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_134.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_135.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_136.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_137.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_138.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_139.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_140.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_141.jpg',\n",
       " 'img_resize/Thang3/2_220474_well02_zid99_30/Thang3_220474_frame_142.jpg']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sequence = [\n",
    "    read_image(img_path, mode=ImageReadMode.RGB)\n",
    "    for img_path in list_path_img\n",
    "]\n",
    "images = torch.stack(image_sequence, dim=0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(images.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:1')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out, 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def delete_augemetation():\n",
    "    i = 0\n",
    "    folder_path = glob('img_resize/*/**')\n",
    "    for path in folder_path:\n",
    "        if 'augmentation' in path:\n",
    "            shutil.rmtree(path)\n",
    "            print(f'Đã xóa thư mục: {path}')\n",
    "            i+=1\n",
    "    print(i)\n",
    "\n",
    "delete_augemetation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqImageDataset(Dataset):\n",
    "    def __init__(self, X, y, transforms=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "        self.image_paths = [\n",
    "            sorted(glob(os.path.join(dir_path, '*.jpg')), key=self.custom_sort_key)\n",
    "            for dir_path in self.X\n",
    "        ]\n",
    "        print(f'Loaded {len(self.image_paths)} sequences')\n",
    "    \n",
    "    @staticmethod\n",
    "    def custom_sort_key(filename):\n",
    "        match = re.search(r'frame_(\\d+)', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return filename  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_sequence = [\n",
    "            read_image(img_path, mode=ImageReadMode.RGB)\n",
    "            for img_path in self.image_paths[idx]\n",
    "        ]\n",
    "        images = torch.stack(image_sequence, dim=0)\n",
    "        label = torch.tensor(self.y.iloc[idx])\n",
    "        if self.transforms:\n",
    "            images = self.transforms(images)\n",
    "        return images, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "    sequences, labels = zip(*batch)\n",
    "    max_len = max([s.shape[0] for s in sequences])\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        seq_len = seq.shape[0]\n",
    "        if seq_len < max_len:\n",
    "            last_frame = seq[-1].unsqueeze(0)\n",
    "            num_repeat = max_len - seq_len\n",
    "            padding = last_frame.repeat(num_repeat, 1, 1, 1)\n",
    "            padded = torch.cat([seq, padding], dim=0)\n",
    "        else:\n",
    "            padded = seq\n",
    "        padded_seqs.append(padded)\n",
    "    try:\n",
    "        padded_seqs = torch.stack(padded_seqs, dim=0)\n",
    "    except:\n",
    "        print('error')\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_test_set(X, y, n_samples_per_class=10):\n",
    "    indices_class_0 = np.where(y == 0)[0]\n",
    "    indices_class_1 = np.where(y == 1)[0]\n",
    "\n",
    "    test_indices_class_0 = np.random.choice(indices_class_0, n_samples_per_class, replace=False)\n",
    "    test_indices_class_1 = np.random.choice(indices_class_1, n_samples_per_class, replace=False)\n",
    "\n",
    "    test_indices = np.concatenate([test_indices_class_0, test_indices_class_1])\n",
    "\n",
    "    mask = np.zeros(len(y), dtype=bool)\n",
    "    mask[test_indices] = True\n",
    "\n",
    "    X_test, X_remainder = X[mask], X[~mask]\n",
    "    y_test, y_remainder = y[mask], y[~mask]\n",
    "\n",
    "    return X_remainder, X_test, y_remainder, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, batch_size, class_0_weight=3):\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.class_0_weight = class_0_weight\n",
    "        self.idx_0 = np.where(self.labels == 0)[0]\n",
    "        self.idx_1 = np.where(self.labels == 1)[0]\n",
    "        self.num_0 = len(self.idx_0)\n",
    "        self.num_1 = len(self.idx_1)\n",
    "        self.start_0 = 0\n",
    "        self.start_1 = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.num_1 + self.batch_size//(self.class_0_weight + 1) - 1) // (self.batch_size // (self.class_0_weight + 1))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.idx_0)\n",
    "        np.random.shuffle(self.idx_1)\n",
    "        \n",
    "        max_batches = len(self)\n",
    "    \n",
    "        for i in range(max_batches):\n",
    "            batch = []\n",
    "            \n",
    "            start_0 = self.start_0\n",
    "            end_0 = start_0 + (self.batch_size * self.class_0_weight) // (self.class_0_weight + 1)\n",
    "            if end_0 > self.num_0:\n",
    "                batch.extend(self.idx_0[start_0: self.num_0])\n",
    "                lack_0 = end_0 - self.num_0\n",
    "                batch.extend(self.idx_0[:lack_0])\n",
    "                self.start_0 = lack_0\n",
    "            else:\n",
    "                batch.extend(self.idx_0[start_0:end_0])\n",
    "                self.start_0 = end_0 % self.num_0\n",
    "            \n",
    "            start_1 = self.start_1\n",
    "            end_1 = start_1 + self.batch_size // (self.class_0_weight + 1)\n",
    "            if end_1 > self.num_1:\n",
    "                batch.extend(self.idx_1[start_1: self.num_1])\n",
    "                lack_1 = end_1 - self.num_1\n",
    "                batch.extend(self.idx_1[:lack_1])\n",
    "                self.start_1 = lack_1\n",
    "            else:\n",
    "                batch.extend(self.idx_1[start_1:end_1])\n",
    "                self.start_1 = end_1 % self.num_1\n",
    "                \n",
    "            np.random.shuffle(batch)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('img_resize/img_resize.csv')\n",
    "X, y = df['img_path'], df['label']\n",
    "X_remainder, X_test, y_remainder, y_test = get_stratified_test_set(X, y, n_samples_per_class=7)\n",
    "X_train, X_val, y_train, y_val = get_stratified_test_set(X_remainder, y_remainder, n_samples_per_class=10)\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:label\n",
      "1    68\n",
      "0    12\n",
      "Name: count, dtype: int64\n",
      "y_val:label\n",
      "1    10\n",
      "0    10\n",
      "Name: count, dtype: int64\n",
      "y_test:label\n",
      "1    7\n",
      "0    7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'y_train:{y_train.value_counts()}\\ny_val:{y_val.value_counts()}\\ny_test:{y_test.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_func(img_path, select: int):\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if select == 0:\n",
    "        # Flip horizontally\n",
    "        augmented = cv2.flip(img, 1)\n",
    "    elif select == 1:\n",
    "        # Rotate 90 degrees clockwise\n",
    "        augmented = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif select == 2:\n",
    "        # Add Gaussian noise\n",
    "        noise = np.random.normal(0, 25, img.shape).astype(np.uint8)\n",
    "        augmented = cv2.add(img, noise)\n",
    "    elif select == 3:\n",
    "        # Adjust brightness\n",
    "        brightness = 50\n",
    "        augmented = cv2.add(img, (brightness,brightness,brightness,0))\n",
    "    elif select == 4:\n",
    "        # Apply Gaussian blur\n",
    "        augmented = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    elif select == 5:\n",
    "        # Change color space (to grayscale)\n",
    "        augmented = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    elif select == 6:\n",
    "        # Adjust contrast\n",
    "        contrast = 1.5\n",
    "        augmented = cv2.convertScaleAbs(img, alpha=contrast, beta=0)\n",
    "    elif select == 7:\n",
    "        # Thay đổi độ bão hòa\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        hsv[:,:,1] = hsv[:,:,1] * 1.5  # Tăng độ bão hòa lên 50%\n",
    "        augmented = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    elif select == 8:\n",
    "        # Áp dụng hiệu ứng cartoon\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.medianBlur(gray, 5)\n",
    "        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 9)\n",
    "        color = cv2.bilateralFilter(img, 9, 300, 300)\n",
    "        augmented = cv2.bitwise_and(color, color, mask=edges)\n",
    "    elif select == 9:\n",
    "        # Thay đổi gamma\n",
    "        gamma = 1.5\n",
    "        invGamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        augmented = cv2.LUT(img, table)\n",
    "    elif select == 10:\n",
    "        # Áp dụng hiệu ứng chồng màu (color overlay)\n",
    "        overlay_color = np.random.randint(0, 256, 3).tolist()\n",
    "        overlay = np.full(img.shape, overlay_color, dtype=np.uint8)\n",
    "        augmented = cv2.addWeighted(img, 0.8, overlay, 0.2, 0)\n",
    "    else:\n",
    "        augmented = img  # Trả về ảnh gốc nếu select nằm ngoài phạm vi\n",
    "    \n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort_key(filename):\n",
    "    match = re.search(r'frame_(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return filename  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(X_train, y_train):\n",
    "    list_idx = [i for i, j in enumerate(y_train) if j == 0]\n",
    "    path_imbalanced = [X_train.iloc[i] for i in list_idx]\n",
    "    path_imbalanced\n",
    "    for path in path_imbalanced:\n",
    "        img_paths = sorted(glob(os.path.join(path, '*.jpg')), key=custom_sort_key)\n",
    "        all_selects = np.arange(11)\n",
    "        np.random.shuffle(all_selects)\n",
    "        used = all_selects[:5]\n",
    "        \n",
    "        for select in used:\n",
    "            dir = f'{path}_augmentation_{select}'\n",
    "            if not os.path.exists(dir):\n",
    "                os.makedirs(dir)\n",
    "            for img_path in img_paths:\n",
    "                img_augmentated = aug_func(img_path, select)\n",
    "                file_name = os.path.splitext(os.path.basename(img_path))[0] + '_augmentated'\n",
    "                cv2.imwrite(f'{dir}/{file_name}.jpg', img_augmentated)\n",
    "            X_train = pd.concat([X_train, pd.Series([dir])], ignore_index=True)\n",
    "            y_train = pd.concat([y_train, pd.Series([0])], ignore_index=True)\n",
    "    return X_train, y_train\n",
    "X_train, y_train = augmentation(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:label\n",
      "1    68\n",
      "0    12\n",
      "Name: count, dtype: int64\n",
      "y_val:label\n",
      "1    10\n",
      "0    10\n",
      "Name: count, dtype: int64\n",
      "y_test:label\n",
      "1    7\n",
      "0    7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'y_train:{y_train.value_counts()}\\ny_val:{y_val.value_counts()}\\ny_test:{y_test.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 sequences\n",
      "Loaded 20 sequences\n",
      "Loaded 14 sequences\n"
     ]
    }
   ],
   "source": [
    "transform_pipeline = v2.Compose([\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "train_dataset = SeqImageDataset(X_train, y_train, transforms=transform_pipeline)\n",
    "val_dataset = SeqImageDataset(X_val, y_val, transforms=transform_pipeline)\n",
    "test_dataset = SeqImageDataset(X_test, y_test, transforms=transform_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_sampler = BalancedBatchSampler(y_train, batch_size=4)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, optimizer, loss_fn, device, folder_name):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_hat = model(X)\n",
    "                loss = loss_fn(y_hat, y)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        \n",
    "        epochs_left = epochs - (epoch + 1)\n",
    "        eta_seconds = epochs_left * epoch_duration\n",
    "        eta = str(timedelta(seconds=int(eta_seconds)))\n",
    "        \n",
    "        print(f'Epoch: {epoch + 1:3d}/{epochs:<3d} | '\n",
    "            f'Train Loss: {train_losses[-1]:.20f} | '\n",
    "            f'Val Loss: {val_losses[-1]:.20f} | '\n",
    "            f'Epoch Time: {epoch_duration:<7.2f}s | '\n",
    "            f'ETA: {eta:<8}')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f'Total training time: {str(timedelta(seconds=int(total_time)))}')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs+1), train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'results/{folder_name}/loss_char.jpg')\n",
    "    print(\"Loss chart saved\")\n",
    "    torch.save(model.state_dict(),f'weight/{folder_name}/last.pt')\n",
    "    print(\"Model saved\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, class_names, folder_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    misclassified_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(data_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "            \n",
    "            start_idx = batch_idx * data_loader.batch_size\n",
    "            batch_misclassified = (predicted != y).nonzero(as_tuple=True)[0]\n",
    "            misclassified_indices.extend(start_idx + batch_misclassified.cpu().numpy())\n",
    "    \n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    \n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(f'results/{folder_name}/cfm.jpg')\n",
    "    plt.close()\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': acc,\n",
    "        'confusion_matrix': cm.tolist(), \n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'misclassified_indices': misclassified_indices\n",
    "    }\n",
    "    \n",
    "    with open(f'results/{folder_name}/evaluation_metrics.json', 'w') as json_file:\n",
    "        json.dump(results, json_file, indent=4, cls=NumpyEncoder)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/100 | Train Loss: 0.59805379282025727949 | Val Loss: 0.81325423717498779297 | Epoch Time: 619.37 s | ETA: 17:01:57\n",
      "Epoch:   2/100 | Train Loss: 0.56325908092891463497 | Val Loss: 0.81325403849283850466 | Epoch Time: 642.66 s | ETA: 17:29:40\n",
      "Epoch:   3/100 | Train Loss: 0.56326959501294526778 | Val Loss: 0.85491987069447838987 | Epoch Time: 627.42 s | ETA: 16:54:19\n",
      "Epoch:   4/100 | Train Loss: 0.56327301439116983506 | Val Loss: 0.77159355084101355349 | Epoch Time: 635.56 s | ETA: 16:56:53\n",
      "Epoch:   5/100 | Train Loss: 0.56326235129552726377 | Val Loss: 0.85492686430613196169 | Epoch Time: 621.24 s | ETA: 16:23:37\n",
      "Epoch:   6/100 | Train Loss: 0.56325056272394513623 | Val Loss: 0.81325767437616980349 | Epoch Time: 675.56 s | ETA: 17:38:22\n",
      "Epoch:   7/100 | Train Loss: 0.56325470699983481992 | Val Loss: 0.81325562795003258909 | Epoch Time: 620.50 s | ETA: 16:01:46\n",
      "Epoch:   8/100 | Train Loss: 0.56323288907023039140 | Val Loss: 0.77158604065577185427 | Epoch Time: 626.91 s | ETA: 16:01:15\n",
      "Epoch:   9/100 | Train Loss: 0.56329798172501954756 | Val Loss: 0.81325860818227135862 | Epoch Time: 623.59 s | ETA: 15:45:46\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(model, train_loader, val_loader, epochs, optimizer, loss_fn, device, 'train2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5000\n",
      "Precision: 0.2500\n",
      "Recall: 0.5000\n",
      "F1-score: 0.3333\n",
      "Number of misclassified samples: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daile/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "class_names = ['0','1']\n",
    "results = evaluate(model, test_loader, device, class_names, 'train2')\n",
    "\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall: {results['recall']:.4f}\")\n",
    "print(f\"F1-score: {results['f1_score']:.4f}\")\n",
    "print(f\"Number of misclassified samples: {len(results['misclassified_indices'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class YMufTTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, test_dataset, batch_size, num_classes, device, lr, folder_name):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.folder_name = folder_name\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=10)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        os.makedirs(f'results/{self.folder_name}', exist_ok=True)\n",
    "        os.makedirs(f'weight/{self.folder_name}', exist_ok=True)\n",
    "    def stat_species(self, dataset):\n",
    "        tke = [[] for _ in range(self.num_classes)]\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            tke[label].append(idx)\n",
    "        return tke\n",
    "    \n",
    "    def arrange_data(self, list_IDs):\n",
    "        return np.array([len(class_data) for class_data in list_IDs])\n",
    "\n",
    "    \n",
    "    def YMufT(self):\n",
    "        A = deepcopy(self.list_IDs)\n",
    "        B = deepcopy(self.lst_ratio)\n",
    "        \n",
    "        if not np.any(self.B_temp):\n",
    "            print('End of data, resetting...')\n",
    "            self.A_temp = deepcopy(self.list_IDs)\n",
    "            self.B_temp = deepcopy(self.lst_ratio)\n",
    "            gc.collect()\n",
    "        \n",
    "        MC = np.where(self.B_temp > 0, self.B_temp, np.inf).argmin()\n",
    "        eps = 0.5 * self.B_temp[MC]\n",
    "        bou1 = np.where(self.B_temp <= self.B_temp[MC] + eps)[0]\n",
    "        bou2 = self.B_temp[bou1]\n",
    "        MB = self.B_temp[bou1[np.argmax(bou2)]]\n",
    "        \n",
    "        F = []\n",
    "        for i in range(self.num_classes):\n",
    "            if self.B_temp[i] > 0:\n",
    "                nt = int(min(self.B_temp[i], MB))\n",
    "                np.random.shuffle(self.A_temp[i])\n",
    "                F.extend(self.A_temp[i][:nt])\n",
    "                del self.A_temp[i][:nt]\n",
    "                self.B_temp[i] -= nt\n",
    "            else:\n",
    "                np.random.shuffle(A[i])\n",
    "                nt = int(min(B[i], MB))\n",
    "                F.extend(A[i][:nt])\n",
    "        \n",
    "        return F\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X)\n",
    "            loss = self.loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                outputs = self.model(X)\n",
    "                loss = self.loss_fn(outputs, y)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(all_targets, all_preds)\n",
    "        return val_loss, val_acc\n",
    "    \n",
    "def train(self, num_loop_eps, total_fold, epochs):\n",
    "        self.list_IDs = self.stat_species(self.train_dataset)\n",
    "        self.lst_ratio = self.arrange_data(self.list_IDs)\n",
    "        self.A_temp = deepcopy(self.list_IDs)\n",
    "        self.B_temp = deepcopy(self.lst_ratio)\n",
    "        \n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for training_period in range(num_loop_eps, 0, -1):\n",
    "            for fold in range(total_fold):\n",
    "                print(f'Training period: {num_loop_eps - training_period + 1}, fold: {fold + 1}')\n",
    "                \n",
    "                F = self.YMufT()\n",
    "                fold_dataset = torch.utils.data.Subset(self.train_dataset, F)\n",
    "                fold_loader = DataLoader(fold_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    epoch_start_time = time.time()\n",
    "                    train_loss = self.train_epoch(fold_loader)\n",
    "                    val_loss, val_acc = self.validate(val_loader)\n",
    "                    \n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                    \n",
    "                    self.scheduler.step(val_loss)\n",
    "                    \n",
    "                    epoch_end_time = time.time()\n",
    "                    epoch_duration = epoch_end_time - epoch_start_time\n",
    "                    epochs_left = epochs * total_fold * num_loop_eps - (epoch + 1 + epochs * (fold + total_fold * (num_loop_eps - training_period)))\n",
    "                    eta_seconds = epochs_left * epoch_duration\n",
    "                    eta = str(timedelta(seconds=int(eta_seconds)))\n",
    "                    print(f'Epoch: {epoch + 1:3d}/{epochs:<3d} | '\n",
    "                          f'Train Loss: {train_loss:.5f} | '\n",
    "                          f'Val Loss: {val_loss:.5f} | '\n",
    "                          f'Val Accuracy: {val_acc:.5f} | '\n",
    "                          f'LR: {self.optimizer.param_groups[0][\"lr\"]:.2e} | '\n",
    "                          f'Epoch Time: {epoch_duration:<7.2f}s | '\n",
    "                          f'ETA: {eta:<8}')\n",
    "                    \n",
    "                    if val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss):\n",
    "                        best_val_acc = val_acc\n",
    "                        best_val_loss = val_loss\n",
    "                        torch.save(self.model.state_dict(), f'weight/{self.folder_name}/best.pt')\n",
    "                        print(f'Model improved: Val Acc: {best_val_acc:.5f}, Val Loss: {best_val_loss:.5f}')\n",
    "                \n",
    "                del fold_dataset, fold_loader\n",
    "                gc.collect()\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f'Total training time: {str(timedelta(seconds=int(total_time)))}')\n",
    "        \n",
    "        # Save the loss chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(train_losses)+1), train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses)+1), val_losses, 'r-', label='Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(f'results/{self.folder_name}/loss_chart.jpg')\n",
    "        print(\"Loss chart saved\")\n",
    "        print(\"Training completed\")\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        self.model.load_state_dict(torch.load('weight/best_cldnn_model_v2.pt'))\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                outputs = self.model(X)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        print(f'Test Accuracy: {accuracy:.5f}')\n",
    "        print(f'Precision: {precision:.5f}')\n",
    "        print(f'Recall: {recall:.5f}')\n",
    "        print(f'F1-score: {f1:.5f}')\n",
    "        print('Confusion Matrix:')\n",
    "        print(cm)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(self.num_classes)\n",
    "        plt.xticks(tick_marks, range(self.num_classes))\n",
    "        plt.yticks(tick_marks, range(self.num_classes))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "# Sử dụng YMufTTrainer\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLDNN(\n",
    "    in_channels=1,\n",
    "    cnn_out_channels=300,\n",
    "    lstm_input_size=300,\n",
    "    lstm_hidden_size=150,\n",
    "    lstm_num_blocks=10,\n",
    "    lstm_num_cells_per_block=10,\n",
    "    dnn_output_size=2,\n",
    "    device=device\n",
    ")\n",
    "model.load_state_dict(torch.load('weight/best_cldnn_model.pt'))\n",
    "trainer = YMufTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=8,\n",
    "    num_classes=2,\n",
    "    device=device,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "trainer.train(num_loop_eps=5, total_fold=3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
